import glob
import re
import os
import pandas as pd

###############
# Input
###############
chip_fastqs_path = config["input"]["ChIP_fastqs"]
if config["input"]["metadata"] != '':
    metadata = pd.read_table(config["input"]["metadata"])

###############
# Parameters
###############
index = config["input"]["bowtie_index"]
process = config["common_params"]["processes"]
include_spikein = config["common_params"]["include_spikein"]
index_primary = config["spikein_params"]["index_primary"]
index_spikein = config["spikein_params"]["index_spikein"]
binsize = config["spikein_params"]["binsize"]

quality = config["samtools_params"]["quality"]
distance = config["samtools_params"]["distance"]

if config["macs2_params"]["broad"]:
    broad_peaks_option = "--broad"
else:
    broad_peaks_option = ""

###############
# Output path
###############
ChIP_result_folder = config["output"]["ChIP_output_path"]

# Check if the directory exist. If not, create the directory
if not os.path.exists(ChIP_result_folder):
    os.makedirs(ChIP_result_folder)

########################
# Some condition check
########################
if include_spikein:
    if index_spikein == "" or index_spikein == None:
        raise ValueError("if you include spikein procedure, then index_spikein need to be spcified")

if index_spikein != "" and index_spikein != None:
    if not include_spikein:
        raise ValueError("Since index_spikein is spcified, <include_spikein> parameter should be set to true")

#############################################
# Create lists of output filenames
#############################################
sample_filenames = glob.glob(chip_fastqs_path+'/*.fastq*')
pattern = r"(_[12])?\.fastq.*"
samples = [re.sub(pattern, '', s.split('/')[-1]) for s in sample_filenames]
samples = list(set(samples))

sam = expand(
        f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.sam",
        sample_name=samples
    )

bam = expand(
        f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.bam",
        sample_name=samples
    )
fixed_bam = expand(
        f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.fixed.bam",
        sample_name=samples
    )

fixed_sort_bam = expand(
        f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.fixed.sort.bam",
        sample_name=samples
    )

dedup_sam = expand(
        f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.dedup.sam",
        sample_name=samples
    )
dedup_bam = expand(
        f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.dedup.bam",
        sample_name=samples
    )
if include_spikein:
    sam_stats = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.spikein.stats",
            sample_name=samples
        )

    primary_sort_bam = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.{index_primary}.sort.bam",
            sample_name=samples
        )

    spikein_sort_bam = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.{index_spikein}.sort.bam",
            sample_name=samples
        )
    if config["input"]["metadata"] == '' :
        bw = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.bw",
            sample_name=samples
        )
        bw_ctrl = []

        peaks_bed = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.{index_primary}.sort_peaks.narrowPeak",
            sample_name=samples
        )
        peaks_bed_wi = []

    else:
        have_input = [s for s in samples if s in metadata['ChIP'].unique()]
        no_input = [s for s in samples if s not in metadata['ChIP'].unique()]
        bw = expand(
                f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.rescale.bw",
                sample_name=have_input
            )
        bw_ctrl = expand(
                f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.bw",
                sample_name=no_input
            )
        peaks_bed = expand(
                f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.{index_primary}.sort_peaks.narrowPeak",
                sample_name=no_input
            )
        peaks_bed_wi = expand(
                f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.{index_primary}.sort.withinput_peaks.narrowPeak",
                sample_name=have_input
            )
else:
    sam_stats = []
    primary_sort_bam = []
    spikein_sort_bam = []
    bw_ctrl = []
    index_primary = "no_primary"
    index_spikein = "no_spikein"
    bw = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.bw",
            sample_name=samples
        )
    if config["input"]["metadata"] == '' :
        peaks_bed = expand(
            f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.dedup_peaks.narrowPeak",
            sample_name=samples
        )
        peaks_bed_wi = []

    else:
        have_input = [s for s in samples if s in metadata['ChIP'].unique()]
        no_input = [s for s in samples if s not in metadata['ChIP'].unique()]
        peaks_bed = expand(
                f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.dedup_peaks.narrowPeak",
                sample_name=no_input
            )
        peaks_bed_wi = expand(
                f"{ChIP_result_folder}/{{sample_name}}/{{sample_name}}.q{quality}.dedup.withinput_peaks.narrowPeak",
                sample_name=have_input
            )

# Set the prefix to activate the conda environment
shell.prefix("source $(conda info --base)/etc/profile.d/conda.sh && conda activate fastq_frip_env && ")
rule all:
    input:
        sam,
        bam,
        fixed_bam,
        fixed_sort_bam,
        dedup_sam,
        dedup_bam,
        sam_stats,
        primary_sort_bam,
        spikein_sort_bam,
        bw,
        bw_ctrl,
        peaks_bed,
        peaks_bed_wi,

########################
# Helper functions
########################
def get_control(sample_name):
    return metadata[metadata.ChIP == sample_name]['Input'].iloc[0]

def get_input(wildcards):
    plain = f"{chip_fastqs_path}/{wildcards.sample}.fastq"
    gzipped = f"{chip_fastqs_path}/{wildcards.sample}.fastq.gz"
    paired1 = f"{chip_fastqs_path}/{wildcards.sample}_1.fastq"
    paired2 = f"{chip_fastqs_path}/{wildcards.sample}_2.fastq"

    if os.path.exists(plain):
        return plain
    elif os.path.exists(gzipped):
        return gzipped
    elif os.path.exists(paired1):
        return [paired1, paired2]

    raise FileNotFoundError(f"No input file found for sample {wildcards.sample}, here's the path{gzipped}, {wildcards.sample}")

##############################
# Alignment starts here
##############################
# alignment
rule bowtie2_map:
    input:
        files = get_input
    wildcard_constraints:
        sample = "[a-zA-Z0-9_-]+"
    threads: process
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.sam"
    shell:
        """
        files=({input.files})
        num_files=${{#files[@]}}

        if [ "$num_files" -eq 1 ]; then
            bowtie2 -p {process} -x {index} -U {input.files} -S {output}
        elif [ "$num_files" -eq 2 ]; then
            bowtie2 -p {process} -x {index} -1 {input.files[0]} -2 {input.files[1]}  -S {output}
        fi
        """

# skip alignments with MAPQ smaller than INT {quality}
rule samtools_filter:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.sam"
    wildcard_constraints:
        sample="[^.]+"
    threads: process
    output:
        temp(f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.bam")
    shell:
        "samtools view --threads {process} -h -q {quality} {input} > {output}"

# sort by reads name and then correctly remove secondary and unmapped reads
rule samtools_fixmate:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.bam"
    params:
        sorted_name = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.sorted.bam"
    wildcard_constraints:
        sample="[^.]+"
    threads: process
    output:
        temp(f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.fixed.bam")
    shell:
        """
        samtools sort -n -@{process} {input} -o {params.sorted_name}
        samtools fixmate -m -r {params.sorted_name} {output}
        rm {params.sorted_name}
        """

# back to sorting by genomic coordinates for markdup
rule samtools_sort:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.fixed.bam"
    wildcard_constraints:
        sample="[^.]+"
    threads: process
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.fixed.sort.bam"
    shell:
        """
        samtools sort -@{process} {input} -o {output}
        samtools index --threads {process} {output}
        """

# remove duplicate reads
rule samtools_markdup:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.fixed.sort.bam"
    params:
        stats_file_path = f"{{maps}}/{{sample_name}}/{{sample}}.markdup.stats"
    wildcard_constraints:
        sample="[^.]+"
    threads: process
    output:
        sam = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.dedup.sam",
        bam = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.dedup.bam"
    shell:
        """
        samtools markdup -f {params.stats_file_path} -r -d {distance} {input} {output.sam}
        samtools view --threads {process} -Sb {output.sam} > {output.bam}
        samtools index --threads {process} {output.bam}
        """

# create an unscaled bigwig file
rule bw:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.dedup.bam"
    wildcard_constraints:
        sample="[^.]+"
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.bw"
    shell:
        "bamCoverage -b {input} -o {output} -of bigwig --binSize {binsize}"


############################################################
# The following procedure is for Spike-in ChIP protocol
############################################################
# Count the number of reads that map to each genome and print the ratio
rule spikein_stats:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.dedup.sam"
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.spikein.stats"
    shell:
        """
        primary_reads=$(( `grep -c '.*{index_primary}.*' {input}` - 22 )) 
        spikein_reads=$(( `grep -c '.*{index_spikein}.*' {input}` - 26 )) 
        echo -e "{index_primary}_reads=$primary_reads" >> {output} 
        echo "{index_spikein}_reads=$spikein_reads" >> {output} 
        echo "ratio of {index_primary} to {index_spikein} reads is" >> {output} 
        echo "scale=2; $primary_reads/$spikein_reads" | bc >> {output} 
        """

# Use grep to create a sam file with only one species reads
rule separate_reads:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.dedup.sam"
    params:
        primary_sam_tmp = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{index_primary}.sam",
        spikein_sam_tmp = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{index_spikein}.sam",
    threads: process
    output:
        output1 = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{index_primary}.sort.bam",
        output2 = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{index_spikein}.sort.bam"
    shell:
        """
        grep -v '.*{index_spikein}.*' {input} > {params.primary_sam_tmp}
        sed -i 's/{index_primary}_chr/chr/' {params.primary_sam_tmp}
        samtools sort {params.primary_sam_tmp} -o {output.output1}
        rm {params.primary_sam_tmp}
        samtools index --threads {process} {output.output1}

        grep -v '.*{index_primary}.*' {input} > {params.spikein_sam_tmp}
        sed -i 's/{index_spikein}_chr/chr/' {params.spikein_sam_tmp}
        samtools sort {params.spikein_sam_tmp} -o {output.output2}
        rm {params.spikein_sam_tmp}
        samtools index --threads {process} {output.output2}
        """

# Spike-in Normalization
rule rescaling:
    input:
        ChIP_stats = f"{{maps}}/{{sample_name}}/{{sample}}.spikein.stats",
        ctrl_stats = lambda wildcards: f"{wildcards.maps}/{get_control(wildcards.sample_name)}/{get_control(wildcards.sample_name)}.spikein.stats",
        ChIP_bam = f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{index_primary}.sort.bam"
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.rescale.bw"
    shell:
        """
        primary_reads=$(awk -F'=' '/{index_primary}_reads/{{print $2}}' {input.ChIP_stats}) 
        spikein_reads=$(awk -F'=' '/{index_spikein}_reads/{{print $2}}' {input.ChIP_stats}) 
        ctrl_primary_reads=$(awk -F'=' '/{index_primary}_reads/{{print $2}}' {input.ctrl_stats})  
        ctrl_spikein_reads=$(awk -F'=' '/{index_spikein}_reads/{{print $2}}' {input.ctrl_stats})
        factor=`echo "scale=20; $ctrl_spikein_reads / $ctrl_primary_reads / $spikein_reads * 15000000" | bc`
        echo "Scaling_factor=$factor" >> {input.ChIP_stats}
        bamCoverage -b {input.ChIP_bam} -o {output} -of bigwig --binSize {binsize} --scaleFactor $factor
        """

##############################
# Calling peaks
##############################
rule bam_to_bed:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{{type}}.bam"
    params:
        output_prefix = f"{{sample}}.q{quality}.{{type}}", 
        output_dir = f"{{maps}}/{{sample_name}}/"
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{{type}}_peaks.narrowPeak"
    shell:
        "macs2 callpeak {broad_peaks_option} -t {input} -n {params.output_prefix} --outdir {params.output_dir}"

rule bam_to_bed_wi:
    input:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{{type}}.bam"
    params:
        output_prefix = f"{{sample}}.q{quality}.{{type}}.withinput", 
        output_dir = f"{{maps}}/{{sample_name}}/",
        ctrl_file = lambda wildcards: f"{wildcards.maps}/{get_control(wildcards.sample_name)}/{get_control(wildcards.sample_name)}.q{quality}.{wildcards.type}.bam",
    output:
        f"{{maps}}/{{sample_name}}/{{sample}}.q{quality}.{{type}}.withinput_peaks.narrowPeak"
    shell:
        "macs2 callpeak {broad_peaks_option} -t {input} -c {params.ctrl_file} -n {params.output_prefix} --outdir {params.output_dir}"